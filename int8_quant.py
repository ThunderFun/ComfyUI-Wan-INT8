import os
import torch
from torch import Tensor, nn
import torch.nn.functional as F

_TRITON_AVAILABLE = False
try:
    import triton
    import triton.language as tl
    _TRITON_AVAILABLE = True
except ImportError:
    pass

_triton_kernels = None
_hadamard_quip_kernels = None

def _get_triton_kernels():
    """Lazy import of triton kernels to avoid import errors when Triton not available."""
    global _triton_kernels
    if _triton_kernels is None and _TRITON_AVAILABLE:
        try:
            from .triton_kernels import triton_int8_linear
            _triton_kernels = triton_int8_linear
        except Exception:
            pass
    return _triton_kernels


def _get_hadamard_quip_kernels():
    """Lazy import of Hadamard-QuIP kernels."""
    global _hadamard_quip_kernels
    if _hadamard_quip_kernels is None and _TRITON_AVAILABLE:
        try:
            from .triton_kernels import triton_hadamard_quip_linear, pytorch_hadamard_quip_linear
            _hadamard_quip_kernels = (triton_hadamard_quip_linear, pytorch_hadamard_quip_linear)
        except Exception:
            pass
    return _hadamard_quip_kernels


_quantize_int8_jit = None
_dequantize_jit = None

def _get_quantize_int8_jit():
    """Lazy initialization of JIT-compiled quantize_int8."""
    global _quantize_int8_jit
    if _quantize_int8_jit is None:
        try:
            @torch.jit.script
            def _quantize_int8_script(x: torch.Tensor, scale: float) -> torch.Tensor:
                return x.float().mul(1.0 / scale).round_().clamp_(-128.0, 127.0).to(torch.int8)
            _quantize_int8_jit = _quantize_int8_script
        except Exception:
            pass
    return _quantize_int8_jit


def _get_dequantize_jit():
    """Lazy initialization of JIT-compiled dequantize."""
    global _dequantize_jit
    if _dequantize_jit is None:
        try:
            @torch.jit.script
            def _dequantize_script(q: torch.Tensor, scale: float) -> torch.Tensor:
                return q.float().mul(scale)
            _dequantize_jit = _dequantize_script
        except Exception:
            pass
    return _dequantize_jit


def quantize_int8(x: torch.Tensor, scale: float | torch.Tensor) -> torch.Tensor:
    """
    Quantize a tensor to INT8 using the provided scale.
    
    Args:
        x: Input tensor (float)
        scale: Scale factor (scalar or per-channel)
    
    Returns:
        Quantized INT8 tensor
    """
    if isinstance(scale, (int, float)):
        jit_fn = _get_quantize_int8_jit()
        if jit_fn is not None:
            try:
                return jit_fn(x, float(scale))
            except Exception:
                pass
    
    return x.float().mul(1.0 / scale).round_().clamp_(-128.0, 127.0).to(torch.int8)


def quantize_int8_chunked(x: torch.Tensor, scale: torch.Tensor, chunk_size: int) -> torch.Tensor:
    """
    Quantize a tensor to INT8 with chunking for memory efficiency.
    
    Args:
        x: Input tensor (float)
        scale: Scale factor (scalar or per-channel)
        chunk_size: Target number of elements per chunk
    
    Returns:
        Quantized INT8 tensor
    """
    total_elements = x.numel()
    
    if total_elements <= chunk_size:
        return quantize_int8(x, scale)
    
    orig_shape = x.shape
    x_2d = x.reshape(-1, x.shape[-1])
    result = torch.empty_like(x_2d, dtype=torch.int8)
    
    chunk_rows = max(1, chunk_size // x_2d.shape[1])
    
    for start_row in range(0, x_2d.shape[0], chunk_rows):
        end_row = min(start_row + chunk_rows, x_2d.shape[0])
        chunk = x_2d[start_row:end_row]
        
        if isinstance(scale, torch.Tensor) and scale.numel() > 1:
            if scale.ndim == 2:
                chunk_scale = scale[start_row:end_row]
            else:
                chunk_scale = scale[start_row:end_row].view(-1, 1) if x_2d.ndim == 2 else scale[start_row:end_row]
        else:
            chunk_scale = scale
        
        result[start_row:end_row] = quantize_int8(chunk, chunk_scale)
    
    return result.reshape(orig_shape)


def _check_nan_inf(tensor: torch.Tensor, name: str, location: str = ""):
    """Check tensor for NaN or Inf values."""
    if not _DEBUG_MODE:
        return
    
    has_nan = torch.isnan(tensor).any().item()
    has_inf = torch.isinf(tensor).any().item()
    
    if has_nan or has_inf:
        loc_str = f" [{location}]" if location else ""
        if has_nan:
            nan_count = torch.isnan(tensor).sum().item()
            print(f"[DEBUG]{loc_str} NaN detected in {name}: {nan_count} values")
        if has_inf:
            inf_count = torch.isinf(tensor).sum().item()
            print(f"[DEBUG]{loc_str} Inf detected in {name}: {inf_count} values")


def dequantize(q: Tensor, scale: float | Tensor, quip_s_u: Tensor | None = None, quip_s_v: Tensor | None = None,
               hadamard_quip: bool = False, hadamard_size_in: int = 0, hadamard_size_out: int = 0,
               sign_row: Tensor | None = None, sign_col: Tensor | None = None) -> Tensor:
    """
    Dequantize INT8 tensor to float.
    
    Args:
        q: Quantized INT8 tensor
        scale: Scale factor (scalar or per-channel)
        quip_s_u: QuIP dense rotation matrix U (for legacy QuIP#)
        quip_s_v: QuIP dense rotation matrix V (for legacy QuIP#)
        hadamard_quip: Whether to use Hadamard-QuIP dequantization
        hadamard_size_in: Size of Hadamard transform for input dimension
        hadamard_size_out: Size of Hadamard transform for output dimension
        sign_row: Row signs for Hadamard-QuIP (diagonal D1)
        sign_col: Column signs for Hadamard-QuIP (diagonal D2)
    
    Returns:
        Dequantized float tensor
    """
    total_elements = q.numel()
    
    if hadamard_quip and (hadamard_size_in > 0 or hadamard_size_out > 0):
        w = q.float() * (scale.view(-1, 1) if isinstance(scale, Tensor) and scale.numel() > 1 else scale)
        
        device = w.device
        dtype = w.dtype
        
        N, K = w.shape
        if hadamard_size_out > 0 and N < hadamard_size_out:
            w = torch.nn.functional.pad(w, (0, 0, 0, hadamard_size_out - N))
        if hadamard_size_in > 0 and K < hadamard_size_in:
            w = torch.nn.functional.pad(w, (0, hadamard_size_in - K, 0, 0))
        
        w = w.T
        
        if hadamard_size_out > 0:
            try:
                from .triton_kernels import triton_hadamard_transform
                w = triton_hadamard_transform(w, normalize=True)
            except Exception:
                w = _pytorch_fwht_simple(w)
        
        if sign_row is not None and hadamard_size_out > 0:
            sign_row_expanded = sign_row[:hadamard_size_out] if sign_row.shape[0] >= hadamard_size_out else sign_row
            w = w * sign_row_expanded.unsqueeze(0)
        
        w = w.T
        
        if hadamard_size_in > 0:
            try:
                from .triton_kernels import triton_hadamard_transform
                w = triton_hadamard_transform(w, normalize=True)
            except Exception:
                w = _pytorch_fwht_simple(w)
        
        if sign_col is not None and hadamard_size_in > 0:
            sign_col_expanded = sign_col[:hadamard_size_in] if sign_col.shape[0] >= hadamard_size_in else sign_col
            w = w * sign_col_expanded.unsqueeze(0)
        
        if hadamard_size_out > 0 and N < hadamard_size_out:
            w = w[:N, :]
        if hadamard_size_in > 0 and K < hadamard_size_in:
            w = w[:, :K]
        
        return w.to(dtype)
    
    if quip_s_u is not None and quip_s_v is not None:
        w = q.float() * (scale.view(-1, 1) if isinstance(scale, Tensor) and scale.numel() > 1 else scale)
        N, K = w.shape
        u = quip_s_u.to(w.dtype)
        v = quip_s_v.to(w.dtype)
        
        if u.numel() == N and v.numel() == K:
            res = u.unsqueeze(1) * w * v.unsqueeze(0)
            return res.squeeze() if q.ndim == 1 else res
        elif u.numel() == N * N and v.numel() == K * K:
            u = u.reshape(N, N)
            v = v.reshape(K, K)
            res = u @ w @ v.T
            return res.squeeze() if q.ndim == 1 else res
        else:
            if _DEBUG_MODE:
                print(f"[QuIP# WARNING] Unexpected sign vector sizes: u={u.shape}, v={v.shape}, w={w.shape}")
            return w

    if isinstance(scale, (int, float)) and total_elements <= CHUNK_THRESHOLD_ELEMENTS:
        jit_fn = _get_dequantize_jit()
        if jit_fn is not None:
            try:
                return jit_fn(q, float(scale))
            except Exception:
                pass

    if total_elements > CHUNK_THRESHOLD_ELEMENTS:
        result = torch.empty_like(q, dtype=torch.float32)
        chunk_rows = max(1, CHUNK_TARGET_ELEMENTS // q.shape[-1])
        
        for start_row in range(0, q.shape[0], chunk_rows):
            end_row = min(start_row + chunk_rows, q.shape[0])
            chunk = q[start_row:end_row]
            
            if isinstance(scale, Tensor) and scale.numel() > 1:
                if scale.ndim == 2:
                    chunk_scale = scale[start_row:end_row]
                else:
                    chunk_scale = scale[start_row:end_row].view(-1, 1) if q.ndim == 2 else scale[start_row:end_row]
            else:
                chunk_scale = scale
            
            result[start_row:end_row] = chunk.float() * chunk_scale
            del chunk
        
        return result
    
    if isinstance(scale, Tensor) and scale.numel() > 1:
        scale = scale.view(-1, 1) if q.ndim == 2 else scale.view(-1)
    return q.float() * scale


def _pytorch_fwht_simple(x: torch.Tensor) -> torch.Tensor:
    """Simple PyTorch FWHT for fallback dequantization."""
    import math
    original_shape = x.shape
    n = original_shape[-1]
    
    x = x.reshape(-1, n)
    batch = x.shape[0]
    
    output = torch.empty_like(x)
    
    h = 1
    while h < n:
        half = n // (2 * h)
        
        x_view = x.view(batch, half, 2, h)
        out_view = output.view(batch, half, 2, h)
        
        a = x_view[:, :, 0, :]
        b = x_view[:, :, 1, :]
        
        out_view[:, :, 0, :] = a + b
        out_view[:, :, 1, :] = a - b
        
        x, output = output, x
        h *= 2
    
    result = x.reshape(original_shape)
    result = result / math.sqrt(n)
    
    return result


def quantize_int8_tensorwise(x: Tensor) -> tuple[Tensor, Tensor]:
    abs_max = x.abs().max()
    scale = (abs_max.float() / 127.0).clamp(min=1e-30)
    return quantize_int8_chunked(x, scale, CHUNK_TARGET_ELEMENTS), scale


def quantize_int8_axiswise(x: Tensor, dim: int) -> tuple[Tensor, Tensor]:
    abs_max = x.abs().amax(dim=dim, keepdim=True)
    scale = (abs_max.float() / 127.0).clamp(min=1e-30)
    return quantize_int8_chunked(x, scale, CHUNK_TARGET_ELEMENTS), scale


def convert_zimage_diffusers_state_dict(sd):
    """Convert z-image state dict from Diffusers INT8 format to ComfyUI format."""
    import re
    new_sd = {}
    attn_groups = {}
    
    print(f"[z-image] Starting state dict conversion for {len(sd)} keys...")
    
    for key, value in sd.items():
        new_key = re.sub(r"all_x_embedder\.\d+-\d+\.", "x_embedder.", key)
        new_key = re.sub(r"all_final_layer\.\d+-\d+\.", "final_layer.", new_key)
        
        if new_key.endswith(".scale_weight"):
            new_key = new_key.replace(".scale_weight", ".weight_scale")
        
        attn_match = re.match(r"(.+\.attention)\.(to_q|to_k|to_v)\.(weight|weight_scale|bias|quip_s_u|quip_s_v)$", new_key)
        if attn_match:
            prefix = attn_match.group(1)
            qkv_type = attn_match.group(2)
            param_type = attn_match.group(3)
            if prefix not in attn_groups:
                attn_groups[prefix] = {}
            attn_groups[prefix][f"{qkv_type}.{param_type}"] = value
            continue
        
        if ".attention.to_out.0." in new_key:
            new_key = new_key.replace(".attention.to_out.0.", ".attention.out.")
        
        if ".attention.norm_q." in new_key:
            new_key = new_key.replace(".attention.norm_q.", ".attention.q_norm.")
        if ".attention.norm_k." in new_key:
            new_key = new_key.replace(".attention.norm_k.", ".attention.k_norm.")
            
        new_sd[new_key] = value
    
    for prefix, params in attn_groups.items():
        if "to_q.weight" in params and "to_k.weight" in params and "to_v.weight" in params:
            q_w, k_w, v_w = params["to_q.weight"], params["to_k.weight"], params["to_v.weight"]
            
            has_quip = any(f"{t}.quip_s_u" in params for t in ["to_q", "to_k", "to_v"])
            
            if has_quip:
                def get_dequant(t_name, w):
                    s = params.get(f"{t_name}.weight_scale", 1.0)
                    u = params.get(f"{t_name}.quip_s_u")
                    v = params.get(f"{t_name}.quip_s_v")
                    
                    if u is None or v is None:
                        return dequantize(w, s).to(torch.bfloat16)
                    
                    return dequantize(w, s, u, v).to(torch.bfloat16)
                
                q_dq = get_dequant("to_q", q_w)
                k_dq = get_dequant("to_k", k_w)
                v_dq = get_dequant("to_v", v_w)
                
                if q_dq.ndim == 1: q_dq = q_dq.unsqueeze(0)
                if k_dq.ndim == 1: k_dq = k_dq.unsqueeze(0)
                if v_dq.ndim == 1: v_dq = v_dq.unsqueeze(0)
                
                new_sd[f"{prefix}.qkv.weight"] = torch.cat([q_dq, k_dq, v_dq], dim=0)
                print(f"[z-image] Dequantized and fused QuIP# QKV for {prefix}. Shapes: Q={q_dq.shape}, K={k_dq.shape}, V={v_dq.shape}, Fused={new_sd[f'{prefix}.qkv.weight'].shape}")
            else:
                new_sd[f"{prefix}.qkv.weight"] = torch.cat([q_w, k_w, v_w], dim=0)
                
                if "to_q.weight_scale" in params and "to_k.weight_scale" in params and "to_v.weight_scale" in params:
                    q_s, k_s, v_s = params["to_q.weight_scale"], params["to_k.weight_scale"], params["to_v.weight_scale"]
                    def to_vec(s, rows):
                        if not isinstance(s, torch.Tensor): return torch.full((rows,), float(s))
                        return torch.full((rows,), float(s.item()), device=s.device, dtype=s.dtype) if s.numel() == 1 else s
                    new_sd[f"{prefix}.qkv.weight_scale"] = torch.cat([to_vec(q_s, q_w.shape[0]), to_vec(k_s, k_w.shape[0]), to_vec(v_s, v_w.shape[0])], dim=0)
            
            if "to_q.bias" in params and "to_k.bias" in params and "to_v.bias" in params:
                new_sd[f"{prefix}.qkv.bias"] = torch.cat([params["to_q.bias"], params["to_k.bias"], params["to_v.bias"]], dim=0)
        else:
            for pk, pv in params.items():
                nk = pk.replace("to_q.", "q.").replace("to_k.", "k.").replace("to_v.", "v.")
                new_sd[f"{prefix}.{nk}"] = pv
    
    return new_sd


def is_diffusers_zimage_format(sd):
    """Check if state dict uses Diffusers-style z-image naming."""
    for key in sd.keys():
        if "all_x_embedder." in key or "all_final_layer." in key or ".attention.to_q." in key:
            return True
    return False


try:
    from torch.compiler import disable as compiler_disable
except ImportError:
    def compiler_disable(fn=None, recursive=True):
        if fn is None:
            return lambda f: f
        return fn

CHUNK_THRESHOLD_ELEMENTS = 67_108_864
CHUNK_TARGET_ELEMENTS = 33_554_432

_DEBUG_MODE = False
_DEBUG_FORWARD = False

if os.environ.get("INT8_DEBUG_MODE", "").lower() in ("1", "true", "yes"):
    _DEBUG_MODE = True

_ENABLE_CUDA_SYNC = os.environ.get("INT8_ENABLE_CUDA_SYNC", "0") == "1"

_CLEAR_CACHE_STRATEGY = os.environ.get("INT8_CLEAR_CACHE", "auto")

_HADAMARD_QUIP_ENABLED = os.environ.get("INT8_HADAMARD_QUIP", "0").lower() in ("1", "true", "yes", "on") and \
                          os.environ.get("INT8_DISABLE_HADAMARD_QUIP", "").lower() not in ("1", "true", "yes")


def set_hadamard_quip_enabled(enabled: bool):
    """
    Enable or disable the Hadamard-QuIP kernel at runtime.
    
    Args:
        enabled: True to enable Hadamard-QuIP, False to disable (use standard W8A8)
    
    Example:
        from int8_quant import set_hadamard_quip_enabled
        set_hadamard_quip_enabled(True)
        set_hadamard_quip_enabled(False)
    """
    global _HADAMARD_QUIP_ENABLED
    _HADAMARD_QUIP_ENABLED = enabled
    status = "enabled" if enabled else "disabled"
    print(f"[INT8] Hadamard-QuIP kernel {status}")


def is_hadamard_quip_enabled() -> bool:
    """Check if Hadamard-QuIP kernel is currently enabled."""
    return _HADAMARD_QUIP_ENABLED


_DISABLE_HADAMARD_QUIP = not _HADAMARD_QUIP_ENABLED


def _should_clear_cache():
    """Check if we should clear CUDA cache based on memory pressure."""
    if _CLEAR_CACHE_STRATEGY == "always":
        return True
    if _CLEAR_CACHE_STRATEGY == "never":
        return False
    if not torch.cuda.is_available():
        return False
    allocated = torch.cuda.memory_allocated()
    reserved = torch.cuda.memory_reserved()
    if reserved > 0:
        utilization = allocated / reserved
        return utilization < 0.5
    return False


def _maybe_synchronize():
    """Synchronize CUDA only if explicitly enabled."""
    if _ENABLE_CUDA_SYNC and torch.cuda.is_available():
        torch.cuda.synchronize()


if _DEBUG_MODE:
    def _log_memory(msg: str, is_forward: bool = False):
        """Log GPU memory usage for OOM debugging."""
        if is_forward and not _DEBUG_FORWARD:
            return
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            max_mem = torch.cuda.max_memory_allocated() / 1024**3
            print(f"[MEM] {msg}: Allocated={allocated:.2f}GB, Reserved={reserved:.2f}GB, Peak={max_mem:.2f}GB")

    def _log_tensor_size(name: str, t: Tensor, is_forward: bool = False):
        """Log tensor size for OOM debugging."""
        if is_forward and not _DEBUG_FORWARD:
            return
        if torch.cuda.is_available():
            size_mb = t.numel() * t.element_size() / 1024**2
            print(f"[TENSOR] {name}: shape={tuple(t.shape)}, dtype={t.dtype}, size={size_mb:.2f}MB")

else:
    def _log_memory(msg: str, is_forward: bool = False):
        pass

    def _log_tensor_size(name: str, t: Tensor, is_forward: bool = False):
        pass


@torch.no_grad()
def int8_forward_dynamic(x: Tensor, weight: Tensor, weight_scale: float | Tensor, bias: Tensor | None, compute_dtype: torch.dtype, chunk_size: int = 0) -> Tensor:
    """Forward with dynamic per-token activation quantization."""
    if chunk_size > 0 and x.shape[0] > chunk_size:
        out = torch.empty((x.shape[0], weight.shape[0]), device=x.device, dtype=compute_dtype)
        for i in range(0, x.shape[0], chunk_size):
            chunk = x[i:i+chunk_size]
            out[i:i+chunk_size] = int8_forward_dynamic(chunk, weight, weight_scale, bias, compute_dtype, chunk_size=0)
            del chunk
        return out

    _log_memory("int8_forward_dynamic start", is_forward=True)
    _log_tensor_size("input x", x, is_forward=True)
    
    _check_nan_inf(x, "input x", "int8_forward_dynamic")
    
    triton_kernels = _get_triton_kernels()
    if triton_kernels is not None and x.ndim >= 2:
        try:
            _log_kernel_usage("w8a8_dynamic_triton", "int8_forward_dynamic")
            return triton_kernels(x, weight, weight_scale, bias, compute_dtype)
        except Exception as e:
            if _DEBUG_MODE:
                print(f"[DEBUG] Triton fallback in int8_forward_dynamic: {e}")
            pass
    
    x_8, x_scale = quantize_int8_axiswise(x, dim=-1)
    res = torch._int_mm(x_8, weight.T)
    del x_8
    
    _log_tensor_size("int_mm result (INT32)", res, is_forward=True)
    
    scale = x_scale * weight_scale
    
    _log_memory("before float conversion (CRITICAL POINT)", is_forward=True)
    
    total_elements = res.numel()
    
    if total_elements > CHUNK_THRESHOLD_ELEMENTS:
        chunk_rows = max(1, (CHUNK_TARGET_ELEMENTS // res.shape[1]))
        
        if _DEBUG_MODE:
            print(f"[CHUNK] Large tensor detected: {res.shape}, processing in chunks of {chunk_rows} rows")
        
        res_scaled = torch.empty_like(res, dtype=compute_dtype)
        
        for start_row in range(0, res.shape[0], chunk_rows):
            end_row = min(start_row + chunk_rows, res.shape[0])
            
            chunk_int32 = res[start_row:end_row]
            
            if scale.numel() == 1 or scale.shape[0] == 1:
                res_scaled[start_row:end_row] = chunk_int32.float().mul_(scale).to(compute_dtype)
            else:
                chunk_scale = scale[start_row:end_row]
                res_scaled[start_row:end_row] = chunk_int32.float().mul_(chunk_scale).to(compute_dtype)
            
            del chunk_int32
        
        if _DEBUG_MODE:
            print(f"[CHUNK] Completed chunked processing")
    else:
        res_scaled = res.float().mul_(scale).to(compute_dtype)
    
    _log_memory("after float conversion", is_forward=True)
    
    del res, scale, x_scale
    
    if bias is not None:
        res_scaled.add_(bias.to(compute_dtype))
    
    _check_nan_inf(res_scaled, "output", "int8_forward_dynamic")
    
    return res_scaled


@torch.no_grad()
def int8_forward_static(x: Tensor, weight: Tensor, weight_scale: float | Tensor, input_scale: float | Tensor, bias: Tensor | None, compute_dtype: torch.dtype, chunk_size: int = 0) -> Tensor:
    """Forward with static (learned) activation quantization."""
    if chunk_size > 0 and x.shape[0] > chunk_size:
        out = torch.empty((x.shape[0], weight.shape[0]), device=x.device, dtype=compute_dtype)
        for i in range(0, x.shape[0], chunk_size):
            chunk = x[i:i+chunk_size]
            out[i:i+chunk_size] = int8_forward_static(chunk, weight, weight_scale, input_scale, bias, compute_dtype, chunk_size=0)
            del chunk
        return out
    
    x_8 = quantize_int8(x, input_scale)
    res = torch._int_mm(x_8, weight.T)
    del x_8
    
    scale = weight_scale * input_scale
    
    total_elements = res.numel()
    
    if total_elements > CHUNK_THRESHOLD_ELEMENTS:
        chunk_rows = max(1, (CHUNK_TARGET_ELEMENTS // res.shape[1]))
        
        res_scaled = torch.empty_like(res, dtype=compute_dtype)
        
        for start_row in range(0, res.shape[0], chunk_rows):
            end_row = min(start_row + chunk_rows, res.shape[0])
            chunk_int32 = res[start_row:end_row]
            res_scaled[start_row:end_row] = chunk_int32.float().mul_(scale).to(compute_dtype)
            del chunk_int32
    else:
        res_scaled = res.float().mul_(scale).to(compute_dtype)
    
    del res
    
    if bias is not None:
        res_scaled.add_(bias.to(compute_dtype))
    
    _check_nan_inf(res_scaled, "output", "int8_forward_static")
    
    return res_scaled


@torch.no_grad()
def chunked_lora_forward(x: Tensor, down: Tensor, up: Tensor, alpha: float, output: Tensor, offset: int = 0, size: int = 0):
    """Memory-efficient float LoRA forward pass using chunking."""
    x_shape = x.shape
    x_2d = x.reshape(-1, x_shape[-1])
    
    chunk_rows = max(1, CHUNK_TARGET_ELEMENTS // max(down.shape[0], up.shape[0]))
    
    if x_2d.shape[0] <= chunk_rows:
        lora_out = F.linear(F.linear(x_2d, down), up)
        
        if size > 0:
            output_view = output.reshape(-1, output.shape[-1])
            if lora_out.shape[1] != size:
                if lora_out.shape[1] > size:
                    lora_out = lora_out[:, :size]
                else:
                    lora_out = torch.nn.functional.pad(lora_out, (0, size - lora_out.shape[1]))
            output_view[:, offset:offset+size].add_(lora_out, alpha=alpha)
        else:
            lora_out = lora_out.reshape(*x_shape[:-1], lora_out.shape[-1])
            output.add_(lora_out, alpha=alpha)
    else:
        output_view = output.reshape(-1, output.shape[-1])
        for i in range(0, x_2d.shape[0], chunk_rows):
            end = min(i + chunk_rows, x_2d.shape[0])
            x_chunk = x_2d[i:end]
            lora_out = F.linear(F.linear(x_chunk, down), up)
            
            if size > 0:
                if lora_out.shape[1] != size:
                    if lora_out.shape[1] > size:
                        lora_out = lora_out[:, :size]
                    else:
                        lora_out = torch.nn.functional.pad(lora_out, (0, size - lora_out.shape[1]))
                output_view[i:end, offset:offset+size].add_(lora_out, alpha=alpha)
            else:
                output_view[i:end].add_(lora_out, alpha=alpha)
            del lora_out
            
            if _ENABLE_CUDA_SYNC and i % (chunk_rows * 4) == 0:
                torch.cuda.synchronize()


@torch.no_grad()
def chunked_int8_lora_forward(x: Tensor, down: Tensor, up: Tensor, down_scale: float | Tensor, up_scale: float | Tensor, alpha: float, output: Tensor, offset: int = 0, size: int = 0):
    """Memory-efficient INT8 LoRA forward pass using chunking."""
    x_shape = x.shape
    x_2d = x.reshape(-1, x_shape[-1])
    
    chunk_rows = max(1, CHUNK_TARGET_ELEMENTS // max(down.shape[0], up.shape[0]))
    
    if x_2d.shape[0] <= chunk_rows:
        x_int8, x_scale = quantize_int8_axiswise(x_2d, dim=-1)
        lora_inter = torch._int_mm(x_int8, down.T)
        del x_int8
        
        lora_inter = lora_inter.to(dtype=torch.float32)
        lora_inter.mul_(x_scale * down_scale)
        
        inter_int8, inter_scale = quantize_int8_axiswise(lora_inter, dim=-1)
        del lora_inter, x_scale
        
        lora_out = torch._int_mm(inter_int8, up.T)
        del inter_int8
        
        lora_out = lora_out.to(dtype=torch.float32)
        lora_out.mul_(inter_scale * up_scale)
        
        if size > 0:
            output_view = output.reshape(-1, output.shape[-1])
            output_view[:, offset:offset+size].add_(lora_out, alpha=alpha)
        else:
            lora_out = lora_out.reshape(*x_shape[:-1], lora_out.shape[-1])
            output.add_(lora_out, alpha=alpha)
        del lora_out, inter_scale
    else:
        output_view = output.reshape(-1, output.shape[-1])
        for i in range(0, x_2d.shape[0], chunk_rows):
            end = min(i + chunk_rows, x_2d.shape[0])
            x_chunk = x_2d[i:end]
            
            x_int8, x_scale = quantize_int8_axiswise(x_chunk, dim=-1)
            lora_inter = torch._int_mm(x_int8, down.T)
            del x_int8
            
            lora_inter = lora_inter.to(dtype=torch.float32)
            lora_inter.mul_(x_scale * down_scale)
            
            inter_int8, inter_scale = quantize_int8_axiswise(lora_inter, dim=-1)
            del lora_inter, x_scale
            
            lora_out = torch._int_mm(inter_int8, up.T)
            del inter_int8
            
            lora_out = lora_out.to(dtype=torch.float32)
            lora_out.mul_(inter_scale * up_scale)
            
            if size > 0:
                output_view[i:end, offset:offset+size].add_(lora_out, alpha=alpha)
            else:
                output_view[i:end].add_(lora_out, alpha=alpha)
                
            del lora_out, inter_scale
            
            if _ENABLE_CUDA_SYNC and i % (chunk_rows * 4) == 0:
                torch.cuda.synchronize()


try:
    from comfy.ops import manual_cast, cast_bias_weight, uncast_bias_weight
    _COMFY_OPS_AVAILABLE = True
except ImportError:
    _COMFY_OPS_AVAILABLE = False

_loading_stats = {"int8_direct": 0, "quantized_on_fly": 0, "excluded": 0, "quantize_time": 0.0}

_inference_kernel_stats = {
    "hadamard_quip_triton": 0,
    "hadamard_quip_pytorch": 0,
    "hadamard_quip_dequant": 0,
    "w8a8_static": 0,
    "w8a8_dynamic": 0,
    "w8a8_dynamic_triton": 0,
    "dequant_fallback": 0,
    "non_quantized": 0,
}

_LOG_KERNEL_PER_LAYER = os.environ.get("INT8_LOG_KERNEL", "0") == "1"

_shown_initial_kernel_info = False


def _log_kernel_usage(kernel_type: str, layer_name: str = ""):
    """Log which kernel is being used for inference."""
    global _shown_initial_kernel_info
    _inference_kernel_stats[kernel_type] = _inference_kernel_stats.get(kernel_type, 0) + 1
    
    if not _shown_initial_kernel_info:
        _shown_initial_kernel_info = True
        _print_initial_kernel_info()
    
    if _LOG_KERNEL_PER_LAYER:
        print(f"[INT8 KERNEL] {kernel_type}: {layer_name}")


def _print_initial_kernel_info():
    """Print one-time info about available kernels at start of inference."""
    print(f"\n[INT8] Inference starting with:")


def print_kernel_summary():
    """Print a summary of kernel usage during inference."""
    total = sum(_inference_kernel_stats.values())
    if total == 0:
        return
    
    print("\n[INT8 KERNEL SUMMARY] Inference kernel usage:")
    print("-" * 50)
    for kernel, count in sorted(_inference_kernel_stats.items(), key=lambda x: -x[1]):
        pct = count / total * 100
        print(f"  {kernel:30s}: {count:5d} ({pct:5.1f}%)")
    print(f"  {'TOTAL':30s}: {total:5d}")
    print("-" * 50)


def reset_kernel_stats():
    """Reset kernel usage statistics."""
    global _shown_initial_kernel_info
    for key in _inference_kernel_stats:
        _inference_kernel_stats[key] = 0
    _shown_initial_kernel_info = False
    print("[INT8] Kernel statistics reset")


def cast_int8_weights(layer, device):
    """Cast INT8 weights to target device, supporting offloading."""
    weight = layer.weight.to(device)
    
    weight_scale = getattr(layer, "weight_scale", None)
    if isinstance(weight_scale, torch.Tensor):
        weight_scale = weight_scale.to(device)
    
    input_scale = getattr(layer, "input_scale", None)
    if isinstance(input_scale, torch.Tensor):
        input_scale = input_scale.to(device)
        
    bias = None
    if getattr(layer, "bias", None) is not None:
        bias = layer.bias.to(device)
        
    return weight, weight_scale, input_scale, bias


if _COMFY_OPS_AVAILABLE:
    class Int8TensorwiseOps(manual_cast):
        """
        Custom ComfyUI operations for INT8 tensorwise quantization.
        
        This properly integrates with ComfyUI's model loading while keeping
        the fast torch._int_mm forward path.
        
        Usage:
            model_options = {"custom_operations": Int8TensorwiseOps}
            model = comfy.sd.load_diffusion_model(path, model_options=model_options)
        """
        excluded_names = []
        offload_to_cpu = False
        chunk_size = 0
        auto_convert_to_int8 = True
        debug_mode = False
        
        class Linear(manual_cast.Linear):
            """Linear layer that directly loads int8 weights and uses fast _int_mm."""
            
            def __init__(self, *args, **kwargs):
                super().__init__(*args, **kwargs)
                self.weight_scale = None
                self.input_scale = None
                self._is_quantized = False
                self._weight_scale_reshaped = False
                self.compute_dtype = torch.bfloat16
                self.comfy_cast_weights = True
                self.lora_patches = []
                
                self._hadamard_quip = False
                self._hadamard_size_in = 0
                self._hadamard_size_out = 0
                self._sign_row = None
                self._sign_col = None
            
            def reset_parameters(self):
                return None
            
            def _load_from_state_dict(
                self,
                state_dict,
                prefix,
                local_metadata,
                strict,
                missing_keys,
                unexpected_keys,
                error_msgs,
            ):
                """Directly load int8 weights and scales from state dict."""
                weight_key = prefix + "weight"
                weight_scale = state_dict.pop(prefix + "weight_scale",
                               state_dict.pop(prefix + "scale",
                               state_dict.pop(prefix + "weight.scale",
                               state_dict.pop(prefix + "weight.weight_scale", 
                               state_dict.pop(prefix + "scale_weight", None)))))
                
                input_scale = state_dict.pop(prefix + "input_scale",
                              state_dict.pop(prefix + "act_scale",
                              state_dict.pop(prefix + "input.scale",
                              state_dict.pop(prefix + "input.input_scale", None))))

                quip_s_u = state_dict.pop(prefix + "quip_s_u", None)
                quip_s_v = state_dict.pop(prefix + "quip_s_v", None)
                
                hadamard_quip = state_dict.pop(prefix + "hadamard_quip", False)
                hadamard_size_in = state_dict.pop(prefix + "hadamard_size_in", 0)
                hadamard_size_out = state_dict.pop(prefix + "hadamard_size_out", 0)
                sign_row = state_dict.pop(prefix + "sign_row", None)
                sign_col = state_dict.pop(prefix + "sign_col", None)
                
                bias_key = prefix + "bias"
                
                state_dict.pop(prefix + "comfy_quant", None)
                
                weight_tensor = state_dict.pop(weight_key, None)
                
                if weight_tensor is not None:
                    if weight_tensor.dtype == torch.int8 and weight_scale is None:
                        print(f"INT8 Loader: WARNING - Found INT8 weight but NO SCALE for {prefix.rstrip('.')}")
                    
                    if Int8TensorwiseOps.debug_mode and weight_scale is not None:
                        if isinstance(weight_scale, torch.Tensor):
                            scale_info = f"tensor{tuple(weight_scale.shape)}"
                            if torch.isnan(weight_scale).any():
                                scale_info += " [NaN DETECTED!]"
                            if torch.isinf(weight_scale).any():
                                scale_info += " [INF DETECTED!]"
                            if (weight_scale == 0).any():
                                scale_info += " [ZERO DETECTED!]"
                        else:
                            scale_info = f"scalar={weight_scale}"
                        print(f"[DEBUG] Loading {prefix.rstrip('.')}: dtype={weight_tensor.dtype}, scale={scale_info}")
                    
                    if weight_tensor.dtype == torch.int8 and weight_scale is not None:
                        is_excluded = any(ex in prefix for ex in Int8TensorwiseOps.excluded_names)
                        is_hadamard_quip = hadamard_quip if isinstance(hadamard_quip, bool) else (hadamard_quip.item() if isinstance(hadamard_quip, torch.Tensor) else False)
                        
                        if is_excluded:
                            self._is_quantized = False
                            if is_hadamard_quip:
                                dequant_weight = dequantize(
                                    weight_tensor, weight_scale,
                                    hadamard_quip=True,
                                    hadamard_size_in=int(hadamard_size_in) if isinstance(hadamard_size_in, torch.Tensor) else hadamard_size_in,
                                    hadamard_size_out=int(hadamard_size_out) if isinstance(hadamard_size_out, torch.Tensor) else hadamard_size_out,
                                    sign_row=sign_row,
                                    sign_col=sign_col
                                ).to(torch.bfloat16)
                            elif quip_s_u is not None:
                                dequant_weight = dequantize(weight_tensor, weight_scale, quip_s_u, quip_s_v).to(torch.bfloat16)
                            else:
                                dequant_weight = dequantize(weight_tensor, weight_scale).to(torch.bfloat16)
                            self.weight = nn.Parameter(dequant_weight, requires_grad=False)
                            _loading_stats["excluded"] += 1
                            if is_hadamard_quip:
                                print(f"INT8 Loader: Dequantized Hadamard-QuIP layer {prefix.rstrip('.')}")
                            elif quip_s_u is not None:
                                print(f"INT8 Loader: Dequantized QuIP# layer {prefix.rstrip('.')}")
                        elif is_hadamard_quip:
                            if not _HADAMARD_QUIP_ENABLED:
                                self._is_quantized = True
                                self._hadamard_quip = False
                                dequant_weight = dequantize(
                                    weight_tensor, weight_scale,
                                    hadamard_quip=True,
                                    hadamard_size_in=int(hadamard_size_in) if isinstance(hadamard_size_in, torch.Tensor) else hadamard_size_in,
                                    hadamard_size_out=int(hadamard_size_out) if isinstance(hadamard_size_out, torch.Tensor) else hadamard_size_out,
                                    sign_row=sign_row,
                                    sign_col=sign_col
                                )
                                q_weight, q_scale = quantize_int8_tensorwise(dequant_weight)
                                self.weight = nn.Parameter(q_weight, requires_grad=False)
                                weight_scale = q_scale
                                _loading_stats["int8_direct"] += 1
                                print(f"INT8 Loader: Converted Hadamard-QuIP layer {prefix.rstrip('.')} to standard W8A8")
                            else:
                                self._is_quantized = True
                                self._hadamard_quip = True
                                self._hadamard_size_in = int(hadamard_size_in) if isinstance(hadamard_size_in, torch.Tensor) else hadamard_size_in
                                self._hadamard_size_out = int(hadamard_size_out) if isinstance(hadamard_size_out, torch.Tensor) else hadamard_size_out
                                if sign_row is not None:
                                    if "_sign_row" in self.__dict__:
                                        del self.__dict__["_sign_row"]
                                    self.register_buffer("_sign_row", sign_row)
                                if sign_col is not None:
                                    if "_sign_col" in self.__dict__:
                                        del self.__dict__["_sign_col"]
                                    self.register_buffer("_sign_col", sign_col)
                                self.weight = nn.Parameter(weight_tensor, requires_grad=False)
                                _loading_stats["int8_direct"] += 1
                                print(f"INT8 Loader: Loaded Hadamard-QuIP layer {prefix.rstrip('.')} (kept INT8)")
                        elif quip_s_u is not None:
                            self._is_quantized = False
                            dequant_weight = dequantize(weight_tensor, weight_scale, quip_s_u, quip_s_v).to(torch.bfloat16)
                            self.weight = nn.Parameter(dequant_weight, requires_grad=False)
                            _loading_stats["excluded"] += 1
                            print(f"INT8 Loader: Dequantized QuIP# layer {prefix.rstrip('.')}")
                        else:
                            self._is_quantized = True
                            self.weight = nn.Parameter(weight_tensor, requires_grad=False)
                            _loading_stats["int8_direct"] += 1
                        
                        if weight_scale is not None:
                            if hasattr(self, "weight_scale"):
                                if "weight_scale" in self._buffers:
                                    del self._buffers["weight_scale"]
                                else:
                                    del self.weight_scale
                            
                            if isinstance(weight_scale, torch.Tensor):
                                if weight_scale.numel() == self.out_features:
                                    weight_scale = weight_scale.float().reshape(1, -1)
                                else:
                                    weight_scale = weight_scale.float().mean().view(())
                            else:
                                weight_scale = torch.tensor(weight_scale, dtype=torch.float32)
                            
                            self.register_buffer("weight_scale", weight_scale)
                            self._weight_scale_reshaped = True
                        
                        if input_scale is not None:
                            if hasattr(self, "input_scale"):
                                if "input_scale" in self._buffers:
                                    del self._buffers["input_scale"]
                                else:
                                    del self.input_scale
                            
                            if isinstance(input_scale, torch.Tensor):
                                input_scale = input_scale.float().mean().view(())
                            else:
                                input_scale = torch.tensor(input_scale, dtype=torch.float32)
                                
                            self.register_buffer("input_scale", input_scale)
                    elif weight_tensor.dtype in (torch.float16, torch.bfloat16, torch.float32):
                        if not Int8TensorwiseOps.auto_convert_to_int8:
                            self._is_quantized = False
                            self.weight = nn.Parameter(weight_tensor, requires_grad=False)
                        else:
                            is_excluded = any(ex in prefix for ex in Int8TensorwiseOps.excluded_names)
                            is_dim1 = self.in_features == 1 or self.out_features == 1 or weight_tensor.ndim == 1
                            
                            if is_excluded or is_dim1:
                                reason = "excluded" if is_excluded else "dim1/1D"
                                if Int8TensorwiseOps.debug_mode:
                                    print(f"Skipping dynamic quantization for {prefix.rstrip('.')} ({reason})")
                                self._is_quantized = False
                                self.weight = nn.Parameter(weight_tensor, requires_grad=False)
                                _loading_stats["excluded"] += 1
                            else:
                                if Int8TensorwiseOps.debug_mode:
                                    print(f"Auto-converting to INT8: {prefix.rstrip('.')} ({weight_tensor.dtype} -> INT8)")
                                
                                import time as _time_mod
                                _q_start = _time_mod.perf_counter()
                                q_weight, q_scale = quantize_int8_tensorwise(weight_tensor)
                                self.weight = nn.Parameter(q_weight, requires_grad=False)
                                self.weight_scale = q_scale
                                self._is_quantized = True
                                _loading_stats["quantized_on_fly"] += 1
                                _loading_stats["quantize_time"] += _time_mod.perf_counter() - _q_start
                    else:
                        self._is_quantized = False
                        self.weight = nn.Parameter(weight_tensor, requires_grad=False)
                else:
                    missing_keys.append(weight_key)
                
                bias_tensor = state_dict.pop(bias_key, None)
                if bias_tensor is not None:
                    self.bias = nn.Parameter(bias_tensor, requires_grad=False)
                else:
                    self.bias = None
                
                for k in list(missing_keys):
                    if k.startswith(prefix) and (k.endswith(".weight_scale") or k.endswith(".input_scale")):
                        missing_keys.remove(k)
                
                for k in list(unexpected_keys):
                    if k.startswith(prefix) and (".quip_s_u" in k or ".quip_s_v" in k):
                        unexpected_keys.remove(k)
            
            def forward(self, x: Tensor) -> Tensor:
                """Fast forward using torch._int_mm for quantized weights."""
                if Int8TensorwiseOps.debug_mode and self._is_quantized:
                    if self.weight_scale is not None:
                        if isinstance(self.weight_scale, torch.Tensor):
                            if torch.isnan(self.weight_scale).any():
                                print(f"[DEBUG FORWARD] NaN weight_scale in {self}! Input shape: {x.shape}")
                            if torch.isinf(self.weight_scale).any():
                                print(f"[DEBUG FORWARD] Inf weight_scale in {self}! Input shape: {x.shape}")
                            if (self.weight_scale == 0).any():
                                print(f"[DEBUG FORWARD] Zero weight_scale in {self}! Input shape: {x.shape}")
                
                if not self._is_quantized:
                    _log_kernel_usage("non_quantized", self.__class__.__name__)
                    if _COMFY_OPS_AVAILABLE:
                        weight, bias, offload_stream = cast_bias_weight(
                            self, x, offloadable=True
                        )
                        out = F.linear(x, weight, bias)
                        uncast_bias_weight(self, weight, bias, offload_stream)
                    else:
                        weight = self.weight.to(x.device, dtype=x.dtype)
                        bias = getattr(self, "bias", None)
                        if bias is not None:
                            bias = bias.to(x.device, dtype=x.dtype)
                        out = F.linear(x, weight, bias)
                    
                    lora_patches = getattr(self, "lora_patches", [])
                    if lora_patches:
                        x_shape = x.shape
                        x_2d = x.reshape(-1, x_shape[-1])
                        
                        for patch_data in lora_patches:
                            if len(patch_data) == 3:
                                down, up, alpha = patch_data
                                down_scale, up_scale = None, None
                                offset, size = 0, 0
                            elif len(patch_data) == 5:
                                down, up, alpha, down_scale, up_scale = patch_data
                                offset, size = 0, 0
                            else:
                                down, up, alpha, down_scale, up_scale, offset, size = patch_data
                            
                            is_int8 = down.dtype == torch.int8 and up.dtype == torch.int8
                            
                            if is_int8 and down_scale is not None and up_scale is not None:
                                x_shape = x.shape
                                x_2d = x.reshape(-1, x_shape[-1])
                                
                                if x_2d.shape[0] > 16:
                                    down_int8 = down.to(device=out.device, non_blocking=True)
                                    up_int8 = up.to(device=out.device, non_blocking=True)
                                    down_scale_t = down_scale.to(device=out.device, non_blocking=True) if isinstance(down_scale, Tensor) else down_scale
                                    up_scale_t = up_scale.to(device=out.device, non_blocking=True) if isinstance(up_scale, Tensor) else up_scale
                                    
                                    chunked_int8_lora_forward(
                                        x_2d, down_int8, up_int8, 
                                        down_scale_t, up_scale_t, 
                                        alpha, out,
                                        offset=offset, size=size
                                    )
                                    
                                    del down_int8, up_int8, down_scale_t, up_scale_t
                                    
                                    if out.numel() > CHUNK_THRESHOLD_ELEMENTS and _should_clear_cache():
                                        torch.cuda.empty_cache()
                                else:
                                    d = dequantize(down, down_scale).to(device=out.device, dtype=out.dtype)
                                    u = dequantize(up, up_scale).to(device=out.device, dtype=out.dtype)
                                    lora_out = F.linear(F.linear(x_2d, d), u)
                                    if size > 0:
                                        out_view = out.reshape(-1, out.shape[-1])
                                        if lora_out.shape[1] != size:
                                            if lora_out.shape[1] > size:
                                                lora_out = lora_out[:, :size]
                                            else:
                                                lora_out = torch.nn.functional.pad(lora_out, (0, size - lora_out.shape[1]))
                                        out_view[:, offset:offset+size].add_(lora_out, alpha=alpha)
                                    else:
                                        lora_out = lora_out.reshape(*out.shape)
                                        out.add_(lora_out, alpha=alpha)
                                    del d, u, lora_out
                            else:
                                d = down.to(device=out.device, dtype=out.dtype)
                                u = up.to(device=out.device, dtype=out.dtype)
                                
                                chunked_lora_forward(x_2d, d, u, alpha, out, offset=offset, size=size)
                                
                                del d, u
                    return out
                
                compute_dtype = x.dtype if x.dtype in (torch.float16, torch.bfloat16) else torch.bfloat16
                
                x_shape = x.shape
                x_2d = x.reshape(-1, x_shape[-1])
                
                weight, weight_scale, input_scale, bias = cast_int8_weights(self, x_2d.device)
                
                if self._hadamard_quip and x_2d.shape[0] > 16:
                    hadamard_kernels = _get_hadamard_quip_kernels()
                    if hadamard_kernels is not None:
                        triton_hadamard_quip_linear, pytorch_hadamard_quip_linear = hadamard_kernels
                        
                        sign_row = getattr(self, "_sign_row", None)
                        sign_col = getattr(self, "_sign_col", None)
                        
                        N, K = weight.shape
                        needs_padding = False
                        if self._hadamard_size_out > 0 and N < self._hadamard_size_out:
                            needs_padding = True
                        if self._hadamard_size_in > 0 and K < self._hadamard_size_in:
                            needs_padding = True
                        
                        if needs_padding:
                            if _DEBUG_MODE:
                                print(f"[DIAG] Padding weight from {weight.shape} to Hadamard dimensions")
                            if self._hadamard_size_out > 0 and N < self._hadamard_size_out:
                                weight = torch.nn.functional.pad(weight, (0, 0, 0, self._hadamard_size_out - N))
                            if self._hadamard_size_in > 0 and K < self._hadamard_size_in:
                                weight = torch.nn.functional.pad(weight, (0, self._hadamard_size_in - K, 0, 0))
                            if _DEBUG_MODE:
                                print(f"[DIAG] Padded weight shape: {weight.shape}")
                        
                        if sign_row is not None and sign_row.device != x_2d.device:
                            sign_row = sign_row.to(x_2d.device)
                        if sign_col is not None and sign_col.device != x_2d.device:
                            sign_col = sign_col.to(x_2d.device)
                        
                        if _DEBUG_MODE:
                            print(f"[DIAG] Hadamard-QuIP forward for layer")
                            print(f"[DIAG]   x_2d device: {x_2d.device}")
                            print(f"[DIAG]   weight device: {weight.device}")
                            if sign_row is not None:
                                print(f"[DIAG]   sign_row device: {sign_row.device}, shape: {sign_row.shape}")
                            if sign_col is not None:
                                print(f"[DIAG]   sign_col device: {sign_col.device}, shape: {sign_col.shape}")
                        
                        try:
                            _log_kernel_usage("hadamard_quip_triton", self.__class__.__name__)
                            y = triton_hadamard_quip_linear(
                                x_2d, weight, weight_scale, bias, compute_dtype,
                                hadamard_size_in=self._hadamard_size_in,
                                hadamard_size_out=self._hadamard_size_out,
                                sign_row=sign_row,
                                sign_col=sign_col,
                                out_features=self.out_features
                            )
                            if y.shape[-1] != self.out_features:
                                if _DEBUG_MODE:
                                    print(f"[DIAG] Slicing output from {y.shape[-1]} to {self.out_features}")
                                y = y[:, :self.out_features]
                        except Exception as e:
                            print(f"[DIAG FALLBACK] Triton failed with: {type(e).__name__}: {e}")
                            if torch.cuda.is_available():
                                allocated = torch.cuda.memory_allocated() / 1024**3
                                reserved = torch.cuda.memory_reserved() / 1024**3
                                total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                                print(f"[DIAG FALLBACK] Memory at failure: Allocated={allocated:.2f}GB, Reserved={reserved:.2f}GB, Total={total:.2f}GB")
                                print(f"[DIAG FALLBACK] Hadamard sizes: in={self._hadamard_size_in}, out={self._hadamard_size_out}")
                                print(f"[DIAG FALLBACK] Input shape: {x_2d.shape}, weight shape: {weight.shape}")
                            
                            if torch.cuda.is_available():
                                print(f"[DIAG FALLBACK] Clearing CUDA cache before PyTorch fallback...")
                                torch.cuda.empty_cache()
                                allocated_after = torch.cuda.memory_allocated() / 1024**3
                                print(f"[DIAG FALLBACK] After cache clear: Allocated={allocated_after:.2f}GB")
                            
                            if Int8TensorwiseOps.debug_mode:
                                print(f"[DEBUG] Hadamard-QuIP Triton failed, falling back to PyTorch: {e}")
                            try:
                                _log_kernel_usage("hadamard_quip_pytorch", self.__class__.__name__)
                                y = pytorch_hadamard_quip_linear(
                                    x_2d, weight, weight_scale, bias, compute_dtype,
                                    hadamard_size_in=self._hadamard_size_in,
                                    hadamard_size_out=self._hadamard_size_out,
                                    sign_row=sign_row,
                                    sign_col=sign_col,
                                    out_features=self.out_features
                                )
                                if y.shape[-1] != self.out_features:
                                    if _DEBUG_MODE:
                                        print(f"[DIAG] Slicing output from {y.shape[-1]} to {self.out_features}")
                                    y = y[:, :self.out_features]
                                print(f"[DIAG FALLBACK] PyTorch fallback succeeded!")
                            except torch.OutOfMemoryError as oom_e:
                                print(f"[DIAG FALLBACK] PyTorch fallback ALSO FAILED with OOM!")
                                print(f"[DIAG FALLBACK] PyTorch OOM: {oom_e}")
                                raise
                    else:
                        _log_kernel_usage("hadamard_quip_dequant", self.__class__.__name__)
                        sign_row = getattr(self, "_sign_row", None)
                        sign_col = getattr(self, "_sign_col", None)
                        if sign_row is not None and sign_row.device != x_2d.device:
                            sign_row = sign_row.to(x_2d.device)
                        if sign_col is not None and sign_col.device != x_2d.device:
                            sign_col = sign_col.to(x_2d.device)
                        
                        w_float = dequantize(
                            weight, weight_scale,
                            hadamard_quip=True,
                            hadamard_size_in=self._hadamard_size_in,
                            hadamard_size_out=self._hadamard_size_out,
                            sign_row=sign_row,
                            sign_col=sign_col
                        ).to(x.dtype)
                        y = F.linear(x_2d, w_float, bias)
                        del w_float
                elif x_2d.shape[0] > 16:
                    if input_scale is not None:
                        _log_kernel_usage("w8a8_static", self.__class__.__name__)
                        y = int8_forward_static(
                            x_2d, weight, weight_scale,
                            input_scale, bias, compute_dtype,
                            chunk_size=Int8TensorwiseOps.chunk_size
                        )
                    else:
                        _log_kernel_usage("w8a8_dynamic", self.__class__.__name__)
                        y = int8_forward_dynamic(
                            x_2d, weight, weight_scale,
                            bias, compute_dtype,
                            chunk_size=Int8TensorwiseOps.chunk_size
                        )
                else:
                    _log_kernel_usage("dequant_fallback", self.__class__.__name__)
                    if self._hadamard_quip:
                        sign_row = getattr(self, "_sign_row", None)
                        sign_col = getattr(self, "_sign_col", None)
                        if sign_row is not None and sign_row.device != x_2d.device:
                            sign_row = sign_row.to(x_2d.device)
                        if sign_col is not None and sign_col.device != x_2d.device:
                            sign_col = sign_col.to(x_2d.device)
                        
                        w_float = dequantize(
                            weight, weight_scale,
                            hadamard_quip=True,
                            hadamard_size_in=self._hadamard_size_in,
                            hadamard_size_out=self._hadamard_size_out,
                            sign_row=sign_row,
                            sign_col=sign_col
                        ).to(x.dtype)
                    else:
                        w_float = dequantize(weight, weight_scale).to(x.dtype)
                    y = F.linear(x_2d, w_float, bias)
                    del w_float
                
                lora_patches = getattr(self, "lora_patches", [])
                if lora_patches:
                    _log_memory(f"before LoRA application ({len(lora_patches)} patches)", is_forward=True)
                    for idx, patch_data in enumerate(lora_patches):
                        if len(patch_data) == 3:
                            down, up, alpha = patch_data
                            down_scale, up_scale = None, None
                            offset, size = 0, 0
                        elif len(patch_data) == 5:
                            down, up, alpha, down_scale, up_scale = patch_data
                            offset, size = 0, 0
                        else:
                            down, up, alpha, down_scale, up_scale, offset, size = patch_data
                        
                        is_int8 = down.dtype == torch.int8 and up.dtype == torch.int8
                        
                        if is_int8 and down_scale is not None and up_scale is not None:
                            _log_memory(f"INT8 LoRA {idx} start", is_forward=True)
                            
                            if x_2d.shape[0] > 16:
                                down_int8 = down.to(device=y.device, non_blocking=True)
                                up_int8 = up.to(device=y.device, non_blocking=True)
                                down_scale_t = down_scale.to(device=y.device, non_blocking=True) if isinstance(down_scale, Tensor) else down_scale
                                up_scale_t = up_scale.to(device=y.device, non_blocking=True) if isinstance(up_scale, Tensor) else up_scale
                                
                                _log_tensor_size(f"LoRA {idx} down (INT8)", down_int8, is_forward=True)
                                _log_tensor_size(f"LoRA {idx} up (INT8)", up_int8, is_forward=True)
                                
                                chunked_int8_lora_forward(
                                    x_2d, down_int8, up_int8, 
                                    down_scale_t, up_scale_t, 
                                    alpha, y,
                                    offset=offset, size=size
                                )
                                
                                del down_int8, up_int8, down_scale_t, up_scale_t
                                
                                if y.numel() > CHUNK_THRESHOLD_ELEMENTS and _should_clear_cache():
                                    torch.cuda.empty_cache()
                            else:
                                d = dequantize(down, down_scale).to(device=y.device, dtype=y.dtype)
                                u = dequantize(up, up_scale).to(device=y.device, dtype=y.dtype)
                                lora_out = F.linear(F.linear(x_2d, d), u)
                                if size > 0:
                                    y_view = y.reshape(-1, y.shape[-1])
                                    y_view[:, offset:offset+size].add_(lora_out.reshape(-1, size), alpha=alpha)
                                else:
                                    y.add_(lora_out, alpha=alpha)
                                del d, u, lora_out
                            
                            _log_memory(f"INT8 LoRA {idx} end", is_forward=True)
                        else:
                            target_dtype = y.dtype
                            d = down.to(device=y.device, dtype=target_dtype).contiguous()
                            u = up.to(device=y.device, dtype=target_dtype).contiguous()
                            
                            chunked_lora_forward(x_2d, d, u, alpha, y, offset=offset, size=size)
                            
                            del d, u
                    _log_memory("after LoRA application", is_forward=True)

                del weight, weight_scale, input_scale, bias
                
                return y.reshape(*x_shape[:-1], y.shape[-1])
        
        class GroupNorm(manual_cast.GroupNorm):
            def __init__(self, *args, **kwargs):
                super().__init__(*args, **kwargs)
                self.lora_patches = []
        
        class LayerNorm(manual_cast.LayerNorm):
            def __init__(self, *args, **kwargs):
                super().__init__(*args, **kwargs)
                self.lora_patches = []
        
        class Conv2d(manual_cast.Conv2d):
            def __init__(self, *args, **kwargs):
                super().__init__(*args, **kwargs)
                self.lora_patches = []
            
            def forward(self, x):
                out = super().forward(x)
                lora_patches = getattr(self, "lora_patches", [])
                if lora_patches:
                    for patch_data in lora_patches:
                        if len(patch_data) == 3:
                            down, up, alpha = patch_data
                        else:
                            down, up, alpha, _, _ = patch_data
                        
                        d = down.to(device=out.device, dtype=out.dtype)
                        u = up.to(device=out.device, dtype=out.dtype)
                        try:
                            lora_out = F.conv2d(F.conv2d(x, d), u)
                            out.add_(lora_out, alpha=alpha)
                        except Exception:
                            pass
                        del d, u
                return out
        
        class Conv3d(manual_cast.Conv3d):
            def __init__(self, *args, **kwargs):
                super().__init__(*args, **kwargs)
                self.lora_patches = []
        
        class ConvTranspose2d(manual_cast.ConvTranspose2d):
            def __init__(self, *args, **kwargs):
                super().__init__(*args, **kwargs)
                self.lora_patches = []
        
        class Embedding(manual_cast.Embedding):
            def __init__(self, *args, **kwargs):
                super().__init__(*args, **kwargs)
                self.lora_patches = []
        
        @classmethod
        def conv_nd(cls, dims, *args, **kwargs):
            if dims == 2:
                return cls.Conv2d(*args, **kwargs)
            elif dims == 3:
                return cls.Conv3d(*args, **kwargs)
            else:
                raise ValueError(f"unsupported dimensions: {dims}")
